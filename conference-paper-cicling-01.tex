\documentclass[a4paper]{llncs}

\usepackage{amssymb}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{multirow}


\usepackage{url}
\urldef{\mailsa}\path|d11668@alunos.uevora.pt|    
\urldef{\mailsb}\path|tcg@uevora.pt|

\begin{document}
\title{Multilingual author profiling \\using SVMs and linguistic features}
%\\Information Retrieval for Text Bases
\titlerunning{}
\author{Roy Khristopher Bayot\\Teresa Gonçalves, PhD}
\institute{Universidade de Évora, Department of Informatics,\\
Rua Romão Ramalho nº59, 7000-671 Évora, Portugal\\
\mailsa, \mailsb
}

\maketitle
\begin{abstract}
This paper describes various experiments done to investigate author profiling of tweets in 4 different languages – English, Dutch, Italian, and Spanish. Profiling consists of age and gender classification, as well as regression on 5 different personality dimensions – extroversion, stability, agreeability, openness, and conscientiousness. Different sets of features were tested – bag-of-words, text ngrams, and POS ngrams. SVM was used as the classifier.~\textit{Tfidf} worked best for most English tasks while for most of the tasks from the other languages, the combination of the best features worked better. 
\keywords{author profiling, tfidf, normalized term frequency, SVM, POS, ngrams}
\end{abstract}

\section{Introduction}
Author profiling has been of importance in the recent years. From a forensic standpoint for example, it could be used to determine potential suspects by getting linguistic profiles and identifying characteristics. From a business intelligence perspective, companies could target specific people through online advertising. By knowing the profile of the authors, companies would easily find what a specific group of people talk about online and device strategies to advertise to these people. They could also analyze product reviews and know what types of products are liked or disliked by certain people. 

The growth of the internet where text is one of the main forms of communication is one of the reasons for a rising interest in author profiling. Through this growth, various corpora could be extracted, curated, assembled from different sources such as blogs, websites, customer reviews, and even twitter posts. Of course, this presents some problems. For example, people from different countries who use the same online platform such as Twitter or Blogger could behave differently in terms of text usage. This presents a difficulty in profiling. This work tries to take this difficulty into account by studying which kind of features are useful for different languages. 

The aim of this work is to investigate the dataset given in PAN 2015~\cite{rangel:2015} where twitter data from 4 different languages are used to profile an author based on age, gender, and 5 personality traits - agreeability, conscientiousness, extrovertedness, openness, and stability. The four languages are English, Dutch, Italian, and Spanish. There are three different sets of features that are investigated in this work. The first set is the bag of words features in the form of term frequency and term frequency inverse document frequency. The second set are text ngrams. And finally, we also study part of speech ngrams to see if information extracted from these features are useful for characterizing twitter users.
%Author profiling from text has been a topic of interest recently because it permeates through many aspects such as business intelligence. The task has been enabled mostly through the growth of the internet where text is the main form of communication. Various corpora could be extracted, curated, assembled, and texts could come from different sources such as blogs, websites, customer reviews, and even twitter posts.
%
%While author anonymity has been present mostly in the web, using profiling can be useful, especially in aspects such as marketing, advertising, as well as security. Profiling mainly uses such text to determine aspects of the author such as age, gender, and certain personality traits. The idea is that topics or word usage are affected by such aspects. For instance, talking about bands or any trending music would be a topic for teenagers. This is not always easy since some people can always think not on their age, and that would affect the writing. Some people can write fiction and it can be that the text was written from the perspective of someone with a different personality type.
%
%This topic is being tackled by PAN~\cite{rangel:2015} for the past 3 years. In this year, the task was specific to author profiling of twitter users in 4 languages - English, Dutch, Italian, and Spanish, and tasks include profiling for age, gender, and the big five personality traits - agreeability, conscientiousness, extrovertedness, openness, and stability.
%
%This describes several experiments that were used on the given dataset. The experiments are mainly on \textit{tfidf}, normalized term frequency, POS ngrams, and text ngrams.

\section{State of the Art}
One of the first few works on author profiling is that of Argamon et al. in~\cite{argamon2009automatically} where texts are categorized base on gender, age, native language, and personality. For personality, only neuroticism was checked. The corpus comes from different sources. The age and gender have the same corpus taken from blog postings. The native language corpus was taken from International Corpus of Learner English. Personality was taken from essays of pyschology students from University of Texas in Austin. Two types of features were obtained: content-based features and style-based features and Bayesian Multinomial Regression was used as a classifier. Argamon et al. had some interesting results where from the gender task, they were able to achieve 76.1\% accuracy using style and content features. For age task with 3 classes, the accuracy was at 77.7\% also using style and content features. For the native language task, the classifiers were able to achieve 82.3\% using only content features. And finally, in checking for neuroticism, the highest obtained was 65.7\% using only style features. 

There has also been some research that uses datasets collected from social media. A particular example is that of Schler et al. in~\cite{schler2006effects} where writing styles in blogs are related to age and gender. Stylistic and content features were extracted from 71,000 different blogs and a Multi-Class Real Winnow was used to learn the models to classify the blogs. Stylistic features included parts-of-speech tags, function words, hyperlinks, and non-dictionary words. Content features included word unigrams with high information gain. The accuracy achieved was around 80\% for gender classification and 75\% for age identification.  

The work or Argamon et al.~\cite{argamon2009automatically} became the basis for the work in PAN. It is an ongoing project from CLEF with author profiling as one of its tasks. It currently has three editions. In the first edition of PAN~\cite{rangel2013overview} in 2013, the task  was age and gender profiling for English and Spanish blogs. There were a variety of methods used. One set includes content-based features such as bag of words, named entities, dictionary words, slang words, contractions, sentiment words, and emotion words. Another would be stylistic features such as frequencies, punctuations, POS, HTML use, readability measures, and other various statistics. There are also features that are n-grams based, IR-based, and collocations-based. Named entities, sentiment words, emotion words, and slang, contractions and words with character flooding were also considered. After extracting the features, the classifiers that were used were the following - decision trees, Support Vector Machines, logistic regression, Naïve Bayes, Maximum Entropy, Stochastic Gradient Descent, and random forests. The work of Lopez-Monroy in~\cite{lopez2013inaoe} was considered the winner for the task although they placed second for both English and Spanish with an accuracy of 38.13\% and 41.58\% respectively. They used second order representation based on relationships between documents and profiles. The work of Meina et al.~\cite{meina2013ensemble} used collocations and placed first for English with a total accuracy of 38.94\%. On the other hand, the work of Santosh et al. in~\cite{santosh2013author} gave a total accuracy of 42.08\% after using POS features for Spanish.

In PAN 2014~\cite{rangel2014overview}, the task was profiling authors with text from four different sources - social media, twitter, blogs, and hotel reviews. Most of the approaches used in this edition are similar to the previous year. In~\cite{lopezusing}, the method used to represent terms in a space of profiles and then represent the documents in the space of profiles and subprofiles were built using expectation maximization clustering. This is the same method as in the previous year in~\cite{lopez2013inaoe}. In~\cite{maharjansimple}, ngrams were used with stopwords, punctuations, and emoticons retained, and then idf count was also used before placed into a classifier. Liblinear logistic regression returned with the best result. In~\cite{weren6exploring}, different features were used that were related to length (number of characters, words, sentences), information retrieval (cosine similarity, okapi BM25), and readability (Flesch-Kincaid readability, correctness, style). Another approach is to use term vector model representation as in~\cite{villenadaedalus}. For the work of Marquardt et al. in~\cite{marquardt2014age}, they used a combination of content-based features (MRC, LIWC, sentiments) and stylistic features (readability, html tags, spelling and grammatical error, emoticons, total number of posts, number of capitalized letters number of capitalized words). Classifiers also varied for this edition. There was the use of logistic regression, multinomial Naïve Bayes, liblinear, random forests, Support Vector Machines, and decision tables. The method of Lopez-Monroy in~\cite{lopezusing} gave the best result with an average accuracy of 28.95\% on all corpus-types and languages. 

\section{Dataset and Tools}
The dataset for the problem at hand is composed of a set of tweets for 4 different languages - English, Dutch, Italian, and Spanish. It was decided to build a different model for each profiling element: age, gender, and the 5 different personality traits - extroverted, stable, agreeable, open, conscientious. There were 4 categories for the age classification - 18-24, 25-34, 35-49, and 50 and above but for Dutch and Italian, age classification is not possible because there was no data given. For personality traits, the values range from -0.5 to 0.5.

The number of given users varies for each language. There were 152 users for English, 34 for Dutch, 38 for Italian, and 100 for Spanish. Each user has different number of tweets. The dataset is balanced based on gender. 

Processing the data was done through Python using the scikits-learn~\cite{scikit-learn} library. For POS tags, TreeTagger as described in~\cite{schmid1994probabilistic} was used with a python wrapper. 

\section{Methodology}
For this study there were three sets of features that were placed under study - bag-of-words features, text ngrams, and  part of speech tags. The same approach was used for all the tasks which was more or less straightforward: feature extraction, then use the features for either classification or regression, and the evaluation was made through a 10 fold cross validation.
\subsection{Preprocessing}
For each language, xml files from each user are read. Then the tweets taken from each user are extracted and concatenated into one line to form one training example. The examples are then transformed by putting them all in lower case. No stop words are removed. Hashtags, numbers, mentions, shares, and retweets were not processed or transformed to anything else. The resulting file is used for feature extraction.  

\subsection{Feature Extraction}
The following set of features were extracted after preprocessing and the number of features extracted from each set is given in the table~\ref{table:numFeatures}: 

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|cccc|}
\hline
Feature       & English & Dutch & Italian & Spanish \\ \hline
tf            & 26264   & 8569  & 12590   & 24688   \\ %\hline
tfidf         & 26264   & 8569  & 12590   & 24688   \\ %\hline
Text bigrams  & 104435  & 29648 & 36002   & 87313   \\ %\hline
Text trigrams & 147577  & 39286 & 44051   & 122130  \\ %\hline
POS unigrams  & 35      & 39    & 38      & 68      \\ %\hline
POS bigrams   & 895     & 792   & 443     & 1999    \\ \hline
\end{tabular}
\caption{Number of features extracted}
\label{table:numFeatures}
\end{table}

\subsubsection{Bag-of-Words}

Two different bag-of-words features were extracted. The first is normalized term frequency. Terms are normalized by the number of terms in a training example. No terms were discarded. The second one is~\textit{tfidf} where terms were normalized by the inverse document frequency. No terms were also discarded. 

\subsubsection{Text Ngrams}
Text ngrams were the second set of features that was examined. Counts of bigrams and trigrams were taken after preprocessing. No normalization was done and no terms were discarded. 

\subsubsection{POS Ngrams}
Part of speech tags were also another set of features that was examined in this study. It was extracted by using a python wrapper to Schmid's TreeTagger program as detailed in~\cite{schmid1994probabilistic}. Counts for unigrams, bigrams, and the combination of the two were used as features. The English parameter file for TreeTagger has 36 different tags. Italian has 38 tags. Dutch has 41 and Spanish has 75. No terms were discarded.




%\begin{table}[!htbp]
%\centering
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline
%        & tf    & tfidf & Text  bi & Text tri & POS uni & POS bi \\ \hline
%English & 26264 & 26264 & 104435   & 147577   & 35      & 895    \\ \hline
%Dutch   & 8569  & 8569  & 29648    & 39286    & 39      & 792    \\ \hline
%Italian & 12590 & 12590 & 36002    & 44051    & 38      & 443    \\ \hline
%Spanish & 24688 & 24688 & 87313    & 122130   & 68      & 1999   \\ \hline
%\end{tabular}
%\caption{Number of features extracted}
%\label{table:numFeatures}
%\end{table}

\subsection{Learning algorithm}
The learning algorithm used was a Support Vector Machines~\cite{cortes1995support} with a linear kernel and the parameter C chosen to be 1, which was the default setting. No parameter tuning was done in these experiments. The rationale for such is because the goal was to determine the effects of the features to the classification and regression.  
\subsection{Performance metrics}
Accuracy was the metric used for the age and gender task. On the other hand, mean squared error was used for all problems involving personality. The mean squared error is given in equation~\ref{eqn:mse}.

\begin{equation}
MSE =\frac{1}{n} \sum_{i=1}^n \left( y_{pred}-y_{actual}\right)^2 
\label{eqn:mse}
\end{equation}


The objective is to get the maximum accuracy and to get the minimum squared error. 
\section{Experimental results}
\subsection{Experiments}
There were four main experiments. The first one uses two different bag of words features separately - normalized term frequency and~\textit{tfidf}. The second one tries two text ngram features separately - text bigrams and text trigrams. The third one uses three POS ngrams features - POS unigrams, POS bigrams, and a combination of POS unigrams and bigrams. The classification/regression used for all three experiments was an SVM with a linear kernel with the $C$ set to 1. This was done with 10 fold cross validation. Accuracies and mean squared errors were noted and the feature that gave a better result in each type were also noted. These features were then concatenated and used for another classification using the same parameters for the SVM and cross validation as before. 
\subsection{Age Classification}
The result for age classification is given in table~\ref{table:AgeResults}. The values in boldface indicate the highest accuracy.~\textit{Tfidf} features gave an accuracy of 69.1\% for English. It is higher than the normalized term frequency and even the combination of the best features. For Spanish however, the combination of the best features gave the highest accuracy of 72.0\%. Dutch and Italian dont have results since the data set doesnt have any labels on this aspect. 
\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|cccc|}
\hline
\multicolumn{2}{|c|}{Feature}                                                     & English        & Dutch                 & Italian               & Spanish        \\ \hline
\multirow{2}{*}{BoW}                                                   & tf       & 0.395          &                       &                       & 0.460          \\ %\cline{2-6} 
                                                                       & tfidf    & \textbf{0.691} &                       &                       & 0.470          \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Text\\ Ngrams\end{tabular}} & bigrams  & 0.638          &                       &                       & 0.640          \\ %\cline{2-6} 
                                                                       & trigrams & 0.553          &                       &                       & 0.460          \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}POS\\ Ngrams\end{tabular}}  & unigrams & 0.579          &                       &                       & 0.550          \\ %\cline{2-6} 
                                                                       & bigrams  & 0.611          &                       &                       & 0.670          \\ %\cline{2-6} 
                                                                       & uni+bi   & 0.593          &                       &                       & 0.700          \\ \hline
\multicolumn{1}{|l|}{}                                                 & combo    & 0.650          & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \textbf{0.720} \\ \hline
\end{tabular}
\caption{Age classification results}
\label{table:AgeResults}
\end{table}

\subsection{Gender Classification}
The result for gender classification is given in table~\ref{table:GenderResults}. The values in boldface indicate the highest accuracy.~\textit{Tfidf} features gave the highest accuracy result for English tweets with 68.3\%. However for the other languages, the combination of the features gives the best result - 65.8\% for Dutch, 75.8\% for Italian, and 80.0\% for Spanish. Only Italian has a tie with another feature set - combination of POS unigrams and bigrams. 
\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|cccc|}
\hline
\multicolumn{2}{|c|}{Feature}                                                     & English        & Dutch & Italian & Spanish        \\ \hline
\multirow{2}{*}{BoW}                                                   & tf       & 0.511          & 0.450 & 0.617   & 0.550          \\ %\cline{2-6} 
                                                                       & tfidf    & \textbf{0.683} & 0.517 & 0.750   & 0.690          \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Text\\ Ngrams\end{tabular}} & bigrams  & 0.623          & 0.550 & 0.733   & 0.700          \\ %\cline{2-6} 
                                                                       & trigrams & 0.652          & 0.650 & 0.700   & 0.520          \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}POS\\ Ngrams\end{tabular}}  & unigrams & 0.659          & 0.642 & 0.733   & 0.770          \\ %\cline{2-6} 
                                                                       & bigrams  & 0.659          & 0.567 & 0.733   & 0.750          \\ %\cline{2-6} 
                                                                       & uni+bi   & 0.660          & 0.592 & \textbf{0.758}   & 0.780          \\ \hline
                                                                       & combo    & 0.671          & \textbf{0.658} & \textbf{0.758}   & \textbf{0.800} \\ \hline
\end{tabular}
\caption{Gender classification results}
\label{table:GenderResults}
\end{table}

\subsection{Personality Regression}
The results for personality regression are given in tables~\ref{table:ExtrovertedResults},~\ref{table:StableResults},~\ref{table:AgreeableResults},~\ref{table:OpenResults}, and~\ref{table:ConscientiousnessResults}. The values in boldface indicate the lowest mean squared errors. For the extroversion component of the personality, the combination of all the best features gave the best results for Dutch, Italian, and Spanish as seen in table~\ref{table:ExtrovertedResults}. However,~\textit{tfidf} gave the best results for English. 
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|cccc|}
\hline
\multicolumn{2}{|c|}{Feature}                                                     & English        & Dutch          & Italian        & Spanish        \\ \hline
\multirow{2}{*}{BoW}                                                   & tf       & 0.027          & 0.024          & 0.030          & 0.029          \\ %\cline{2-6} 
                                                                       & tfidf    & \textbf{0.023} & 0.016          & 0.019          & 0.025          \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Text\\ Ngrams\end{tabular}} & bigrams  & 0.025          & 0.015          & 0.021          & 0.025          \\ %\cline{2-6} 
                                                                       & trigrams & 0.026          & 0.018          & 0.022          & 0.028          \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}POS\\ Ngrams\end{tabular}}  & unigrams & 0.040          & 0.020          & 0.030          & 0.129          \\ %\cline{2-6} 
                                                                       & bigrams  & 0.047          & 0.015          & 0.015          & 0.023          \\ %\cline{2-6} 
                                                                       & uni+bi   & 0.050          & 0.016          & 0.017          & 0.024          \\ \hline
                                                                       & combo    & 0.029          & \textbf{0.014} & \textbf{0.015} & \textbf{0.022} \\ \hline
\end{tabular}
\caption{Extroversion regression results}
\label{table:ExtrovertedResults}
\end{table}

The results for the stability component of personality are given in table~\ref{table:StableResults}. The results vary for each language.~\textit{Tfidf} features work well for English with a mean squared error of 0.039. The combination of POS unigrams and bigrams work well for Dutch and Italian. But for Italian, the POS bigrams also work just as well as the combination. Finally for Spanish, POS bigrams gave the best results. 
\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|cccc|}
\hline
\multicolumn{2}{|c|}{Feature}                                                     & English        & Dutch          & Italian        & Spanish        \\ \hline
\multirow{2}{*}{BoW}                                                   & tf       & 0.048          & 0.048          & 0.031          & 0.046          \\ %\cline{2-6} 
                                                                       & tfidf    & \textbf{0.039} & 0.028          & 0.030          & 0.034          \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Text\\ Ngrams\end{tabular}} & bigrams  & 0.045          & 0.031          & 0.031          & 0.038          \\ %\cline{2-6} 
                                                                       & trigrams & 0.048          & 0.041          & 0.033          & 0.041          \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}POS\\ Ngrams\end{tabular}}  & unigrams & 0.055          & 0.030          & 0.040          & 0.135          \\ %\cline{2-6} 
                                                                       & bigrams  & 0.057          & 0.027          & \textbf{0.020} & \textbf{0.032} \\ %\cline{2-6} 
                                                                       & uni+bi   & 0.059          & \textbf{0.023} & \textbf{0.020} & 0.033          \\ \hline
                                                                       & combo    & 0.044          & 0.027          & 0.023          & 0.034          \\ \hline
\end{tabular}
\caption{Stability regression results}
\label{table:StableResults}
\end{table}

The results for the agreeability component of personality are given in table~\ref{table:AgreeableResults}. For this set,~\textit{tfidf} seems to work for English, Dutch, and Spanish, except that Spanish~\textit{tfidf} results tie with results from the combination of the best features. The combination of the best features also works well for Italian.

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|cccc|}
\hline
\multicolumn{2}{|c|}{Feature}                                                     & English        & Dutch          & Italian        & Spanish        \\ \hline
\multirow{2}{*}{BoW}                                                   & tf       & 0.025          & 0.025          & 0.026          & 0.028          \\ %\cline{2-6} 
                                                                       & tfidf    & \textbf{0.023} & \textbf{0.024} & 0.022          & \textbf{0.024} \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Text\\ Ngrams\end{tabular}} & bigrams  & 0.027          & 0.030          & 0.026          & 0.027          \\ %\cline{2-6} 
                                                                       & trigrams & 0.026          & 0.026          & 0.026          & 0.028          \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}POS\\ Ngrams\end{tabular}}  & unigrams & 0.031          & 0.041          & 0.039          & 0.222          \\ %\cline{2-6} 
                                                                       & bigrams  & 0.037          & 0.034          & 0.020          & 0.027          \\ %\cline{2-6} 
                                                                       & uni+bi   & 0.038          & 0.036          & 0.024          & 0.029          \\ \hline
                                                                       & combo    & 0.028          & 0.031          & \textbf{0.019} & \textbf{0.024} \\ \hline
\end{tabular}
\caption{Agreeability regression results}
\label{table:AgreeableResults}
\end{table}


The results for the openness component of personality are given in table~\ref{table:OpenResults}. It again shows varied results.~\textit{Tfidf} features gave the best results for English. The combination of POS unigrams and bigrams also tied with~\textit{tfidf} gave the best results for Dutch. For Italian however, three different feature sets gave the same results - POS bigrams, the combination of POS unigrams and bigrams, and finally, the combination of the best features. Finally for Spanish, POS bigrams gave the best results. 

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|cccc|}
\hline
\multicolumn{2}{|c|}{Feature}                                                     & English        & Dutch          & Italian        & Spanish        \\ \hline
\multirow{2}{*}{BoW}                                                   & tf       & 0.023          & 0.019          & 0.015          & 0.036          \\ %\cline{2-6} 
                                                                       & tfidf    & \textbf{0.020} & \textbf{0.012} & 0.014          & 0.026          \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Text\\ Ngrams\end{tabular}} & bigrams  & 0.021          & 0.014          & 0.015          & 0.030          \\ %\cline{2-6} 
                                                                       & trigrams & 0.022          & 0.015          & 0.016          & 0.034          \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}POS\\ Ngrams\end{tabular}}  & unigrams & 0.029          & 0.016          & 0.019          & 0.061          \\ %\cline{2-6} 
                                                                       & bigrams  & 0.035          & 0.013          & \textbf{0.013} & \textbf{0.023} \\ %\cline{2-6} 
                                                                       & uni+bi   & 0.038          & \textbf{0.012} & \textbf{0.013} & 0.024          \\ \hline
                                                                       & combo    & 0.024          & 0.013          & \textbf{0.013} & 0.024          \\ \hline
\end{tabular}
\caption{Openness regression results}
\label{table:OpenResults}
\end{table}

The results for the conscientiousness component of personality are given in table~\ref{table:ConscientiousnessResults}. We again have varied results, POS bigrams and the combination of POS unigrams and bigrams gave the best results for English. The combination of the best features gave the best results for Dutch. POS bigrams gave the results for Italian while~\textit{tfidf} gave the best results for Spanish.

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{Feature}                                                     & English        & Dutch          & Italian        & Spanish        \\ \hline
\multirow{2}{*}{BoW}                                                   & tf       & 0.023          & 0.015          & 0.024          & 0.028          \\ %\cline{2-6} 
                                                                       & tfidf    & 0.018          & 0.012 & 0.022          & \textbf{0.024} \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Text\\ Ngrams\end{tabular}} & bigrams  & 0.019          & 0.012          & 0.022          & 0.026          \\ %\cline{2-6} 
                                                                       & trigrams & 0.020          & 0.015          & 0.023          & 0.028          \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}POS\\ Ngrams\end{tabular}}  & unigrams & 0.019          & 0.061          & 0.027          & 0.083          \\ %\cline{2-6} 
                                                                       & bigrams  & \textbf{0.013} & 0.023          & \textbf{0.015} & 0.028          \\ %\cline{2-6} 
                                                                       & uni+bi   & \textbf{0.013} & 0.024          & 0.017          & 0.033          \\ \hline
                                                                       & combo    & 0.019          & \textbf{0.010} & 0.016          & 0.025          \\ \hline
\end{tabular}
\caption{Conscientiousness regression results}
\label{table:ConscientiousnessResults}
\end{table}

\section{Conclusions and Future Work}
From the current work, we can observe that different features work better for different languages.~\textit{Tfidf} features worked best for all profiling tasks in English except conscientiousness regression. The combination of the best features gives better results for gender classification and extroversion regression for Dutch. However, other features work better for the other tasks - POS unigrams and bigrams for regression on stability,~\textit{tfidf} on agreeability and openness, POS unigrams and bigrams also for openness, and the combination of the best features for conscientiousness. Two types of features gave the best results for the different tasks for Italian - the combination of all the best features and the combination of POS unigrams and bigrams. Finally for Spanish, the combination of all the best features works for most of the tasks - age and gender classification and extroversion and agreeability regression. POS bigrams worked best for stability and openness regression, while~\textit{tfidf} worked best for conscientious regression.

These results are by no means exhaustive. There are still many methods that could be explored. For instance, the preprocessing is quite minimal. More features could be extracted from the text such as number of links, retweets, hashtags, and mentions, length of tweets, ratios of uppercase to lower case characters, non-dictionary words, lexical diversity, emoticons, sentiment words, informative words and character ngrams. Furthermore, since Support Vector Machines with linear kernel was used, it would also be worth exploring other kernels as well as doing parameter tuning. Finally, a multi-dimensional classification approach can also be explored. Instead of having two distinct classes for age and gender classification for instance, the target classes could be the combination of the two classes. Instead of having 5 different regressions on personality traits, it would be possible to take all 5 traits all at once. This is worth exploring since the traits can be codependent. 


%From the current work, we can conclude that in bag of words set of features,~\textit{tfidf} outperforms normalized term frequency. It gives the best result on all tasks and all languages. For text ngrams set of features, the results are more varied. Text bigrams work better than text trigrams on all languages and tasks except Dutch gender classification. Finally POS tags gives varied results. The combination of POS unigrams and bigrams give the best results for Italian and Spanish in age and gender classification but using POS unigrams and bigrams tend to be much better for Dutch and English respectively. In personality tasks however, most of the results tend to favor bigrams. The only exception would be English which has unigrams that gives the best result, and also that the combination of unigrams and bigrams also give the best result for Dutch. However, we can also conclude that combining the features from the best among each of the 3 different sets of features. 
%
%Looking into methods used on each language, we can see that~\textit{tfidf} works well on personality related tasks, except that of Italian where POS bigrams gave the best result. For age and gender classification however, everything is varied.~\textit{Tfidf} works best on English, but it is text trigrams for Dutch, POS unigrams and bigrams for Italian and Spanish. 
%
%There are still many methods that could be explored. For instance, the preprocessing is quite minimal. More features could be extracted from the text such as number of links, retweets, hashtags, and mentions, length of tweets, ratios of uppercase to lower case characters, non-dictionary words, lexical diversity, emoticons, sentiment words, informative words and character ngrams. Furthermore, since Support Vector Machines with linear kernel was used, it would also be worth exploring other kernels as well as doing parameter tuning. Finally, a multi-dimensional classification approach can also be explored. Instead of having two distinct classes for age and gender classification for instance, the target classes could be the combination of the two classes. Instead of having 5 different regressions on personality traits, it would be possible to take all 5 traits all at once. This is worth exploring since the traits can be codependent. 



\bibliographystyle{plain}
\bibliography{citations}

\end{document}